{
    "thought": "After analyzing the existing memory modules, it's clear that while similarity-based retrieval and generative approaches have their merits, there's room for innovation in how memories are utilized during task execution. The proposed 'ContextualMemory' module aims to enhance performance by not only retrieving similar task trajectories but also by dynamically adjusting the agent's approach based on the context of the current task. This involves analyzing the current environment and task requirements in real-time to select the most relevant memories and adapt strategies accordingly. The module will use a combination of task similarity and contextual relevance, evaluated through a prompt-based scoring system, to guide the retrieval process. This approach ensures that the agent not only relies on past successes but also adapts to the nuances of the current scenario.",
    "name": "contextual",
    "code": "class MemoryContextual(MemoryBase):\n    def __init__(self, llms_type) -> None:\n        super().__init__(llms_type, 'contextual')\n\n    def retriveMemory(self, query_scenario):\n        # Extract task name and current environment from query scenario\n        task_name = re.findall(r'Your task is to:\\s*(.*?)\\s*>', query_scenario)[2]\n        current_environment = query_scenario.split('You are in the middle of a room. Looking quickly around you, you see')[1].split('Your task is to:')[0]\n        \n        # Return empty string if memory is empty\n        if self.scenario_memory._collection.count() == 0:\n            return ''\n            \n        # Find most similar memories based on task name\n        similarity_results = self.scenario_memory.similarity_search_with_score(\n            task_name, k=3)\n            \n        # Evaluate each memory's contextual relevance to the current environment\n        contextual_scores = []\n        for result in similarity_results:\n            memory_environment = result[0].metadata['task_trajectory'].split('You are in the middle of a room. Looking quickly around you, you see')[1].split('Your task is to:')[0]\n            \n            # Generate prompt to evaluate contextual relevance\n            prompt = f'''Evaluate how relevant the following memory's environment is to the current environment for completing the task. Provide a score from 1 (not relevant) to 10 (highly relevant).\n\nMemory Environment:\n{memory_environment}\n\nCurrent Environment:\n{current_environment}\n\nScore: '''\n            \n            # Get contextual score\n            response = llm_response(prompt=prompt, model=self.llm_type, temperature=0.1, stop_strs=['\\n'])\n            score = int(re.search(r'\\d+', response).group()) if re.search(r'\\d+', response) else 0\n            contextual_scores.append(score)\n        \n        # Select memory with highest contextual score\n        max_score_idx = contextual_scores.index(max(contextual_scores))\n        return similarity_results[max_score_idx][0].metadata['task_trajectory']\n\n    def addMemory(self, current_situation):\n        # Extract task description and environment\n        task_name = re.search(r'Your task is to:\\s*(.*?)\\s*>', current_situation).group(1)\n        environment = current_situation.split('You are in the middle of a room. Looking quickly around you, you see')[1].split('Your task is to:')[0]\n        \n        # Create document with metadata\n        memory_doc = Document(\n            page_content=task_name,\n            metadata={\n                \"task_name\": task_name,\n                \"task_trajectory\": current_situation,\n                \"environment\": environment\n            }\n        )\n        \n        # Add to memory store\n        self.scenario_memory.add_documents([memory_doc])\n    ",
    "performance": 0.9
}